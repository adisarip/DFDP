\section{Introduction \& Motivation}

In real-time systems it is common for applications to consist of tasks which consume input data in order to perform a particular function. Typically this input is either sensor data that is crucial for determining the state of the system or to be used as input to other tasks. In these types of systems, there is usually an explicit or implicit timeliness requirement for this data. The relevance of a computation can be intuitively evaluated by the age, or ``freshness,'' of the data that was used during the computation.

Traditionally, task sets have been over-engineered to provide fresh data. Assuming a task must perform some computation on its input data (which is produced by another task) every one second, the obvious question is what should be the rate at which the task producing the required data execute? In these situations, engineers may err on the side of caution and choose to produce the input data faster than necessary, which then needs to be tested to check for freshness in the worst-case scheduling scenario. For example, a task that reads a sensor value and forwards it to another task may be dispatched fifty times a second although the consuming task only needs data younger than one tenth of a second for safety. While this is safe, it reduces the efficiency and schedulability of the system, and there is no analytical framework to ensure the freshness constraint. This leads to more dependence on testing and several cycles of parameter tuning to ensure the safety and efficiency of the system, while ultimately providing no guarantee of optimality, as noted by others in the field \cite{BiniNatale,SetoLehoczkySha,ChantemWangLemmonHu,BelwalCheng}. This paper outlines an algebraic interpretation of this over-engineering strategy for 2 and 3 task chains, and then uses insight from these results to build an optimization problem for arbitrary task chains. Our solution provides a strategy for choosing the period of producer tasks with nearly no information about the scheduling algorithm used.

Real-time systems are ones where tasks must be executed in a timely manner for correct results. For real-time systems that require managing and passing data, it is understood that this data may need some notion of timeliness as well. The most intuitive way of timestamping data is to consider when it was created or obtained, e.g. when a sensor polling task reads it.

In broad terms, the goal we wish to achieve is for the input of a given task to meet predetermined freshness guarantees, where freshness represents the age of the data. An example of this would be a task $B$ which needs to use speed sensor data produced by a task $A$ that is at most 100 milliseconds old.

The notion of freshness is not a new concept. A variety of domains consider this notion, with the definition tailored to each domain, all of which agree that the quality of data is not merely a function of its ``correctness'' or accuracy. Common notions use terms such as currency \cite{Segev1990}, which describes how much time has passed since data collection, and timeliness \cite{Wang:1996:BAD:1189570.1189572}, which measures how old the data is when collected. We will consider freshness to be the time elapsed since the data was produced, particularly when it is output by a generation or collection task. In particular, we are going to examine the following scenario: given a task $Z$ with fixed period, which consumes data that makes its way through a task chain $A, B, C, \ldots$, choose periods for $A, B, C, \ldots$ so as to ensure the freshness bound is always enforced. That is to say, when task $Z$ runs, the age of the data created by task $A$ which was used to eventually produce $Z$'s input is less than a given freshness bound.

The solution to this problem is not always unique. Given a set of solutions, we wish to rank them against some metric to select the one that best fits our needs. There are several metrics one could use; in this work we focus on minimizing task set utilization. More precisely, we will attempt to minimize maximum system utilization. We chose this metric because it is a common metric of schedulability and efficiency in the real-time systems domain. Intuitively, a low task set utilization provides the necessary performance at the lowest computational cost, allowing for more tasks to be introduced to the system and increasing the likelihood of schedulability.

We will attempt to solve this problem with minimal assumptions about the system and scheduling policy. This could provide useful for task sets which may run on many hardware and software combinations. In such a case, one schedulable solution could be implemented on several systems with confidence that the freshness constraint will be met, without the need to alter the task set or configure the system, as long as WCETs and BCETs remain somewhat consistent. For example, a company could provide software for ``open'' hardware or systems where users provide custom or third party firmware without the need to tune the software or OS for each user.