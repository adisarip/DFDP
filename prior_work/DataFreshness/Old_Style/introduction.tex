\section{Introduction}

There are many applications where a task needs to consume data in order to perform its duties. In real-time systems, this input is often sensor data that is crucial for determining the behavior of the system. Input may also be from the output of other tasks. In these types of systems, there is usually an explicit or implicit timeliness requested for this data. The relevance of a computation can be intuitively evaluated by the age, or "freshness," of the data that was used during the computation.

Traditionally, tasks have been over-engineered to provide fresh data. Imagine a task that must perform some computation on data, produced by another task, every one second. Should the task producing the required data run every second? 500 milliseconds? 100 milliseconds? How is the designer to decide? In these situations, engineers may err on the side of caution and choose to produce the input data faster than necessary, which they then need to test in order to evaluate safety. For example, A task that reads a sensor value and forwards it to another task may be dispatched fifty times a second even though the consuming task only needs data younger than one tenth of a second for safety. While this is presumably safe, it reduces the efficiency (and possibly schedulability) of the system, and there is no formal method for us to decide if this value is safe regardless. This leads to more dependence on testing. As Microsoft acknowledges, overenginnering alway leads to wasted effort \cite{CODEMINE}. This often leads to several cycles of parameter tuning in order to ensure the safety and efficiency of the system without ultimately guaranteeing optimal efficiency, as noted by several others in the field \cite{BiniNatale,SetoLehoczkySha,ChantemWangLemmonHu,BelwalCheng}. This paper outlines a formalization of this over-engineering strategy and presents results for choosing the period of input tasks given little information about the underlying system.